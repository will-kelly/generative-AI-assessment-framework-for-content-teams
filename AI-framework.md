# Generative AI assessment framework for content teams

This repository contains a practical maturity model and assessment toolkit that helps content teams evaluate how effectively they are adopting and governing generative AI. It is designed for content operations, marketing, documentation, and knowledge management teams that want to move beyond AI hype and measure real impact.

## How to use this repository

You can use this framework in three main ways:

1. As a workshop tool to run a live assessment with your content team
2. As an internal audit to brief leadership on AI readiness and gaps
3. As a recurring checkpoint to track maturity over time

Suggested workflow:

1. Copy this framework into your preferred workspace (Notion, Confluence, internal wiki)
2. Use the scoring model with your team to rate each dimension from 1 to 5
3. Capture notes, survey responses, and KPIs for each area
4. Summarize scores in a heatmap or table and identify priorities
5. Revisit the assessment quarterly to track progress

If you are pairing this repo with slides, a common structure is:

- `/slides/generative_ai_assessment_framework_content_teams.pptx` for stakeholder presentations
- `/assessments/` for captured results over time
- `/templates/` for any custom questionnaires or dashboards

## Assessment dimensions

The framework evaluates generative AI adoption across eight dimensions:

1. Strategic alignment  
2. Capability readiness  
3. Content quality and safeguards  
4. Productivity and efficiency  
5. Innovation and creativity  
6. Governance and compliance  
7. Adoption and culture  
8. Metrics and reporting  

Each dimension has:

- A scoring rubric from 1 to 5
- Example survey prompts to use with your team
- Suggested KPIs to track over time

---

## 1. Strategic alignment

Does your generative AI usage clearly support your content and business strategy?

**Scoring rubric**

- 1: No AI vision. No clear reason why AI is being used in content work.  
- 2: Individuals experiment with AI without shared goals.  
- 3: Some defined use cases, but they are not consistently tied to outcomes.  
- 4: Most AI use cases are aligned with campaign or content objectives.  
- 5: Clear, documented AI strategy that ties directly to business and content outcomes.

**Example survey prompts**

- Do you understand why our team is using generative AI?  
- Are AI use cases explicitly mapped to content or business goals?  
- Do you know where AI is not allowed or discouraged in our workflows?

**Suggested KPIs**

- Percentage of AI use cases that have a documented business or content objective  
- Number of prohibited or restricted AI use cases explicitly documented  
- Number of AI use cases linked to specific campaigns or product initiatives  

---

## 2. Capability readiness

Do people have the tools, access, and skills they need to use AI effectively?

**Scoring rubric**

- 1: No access to AI tools.  
- 2: Limited access through personal or free accounts.  
- 3: AI tools available, but not integrated into systems or workflows.  
- 4: AI tools integrated into core content workflows for many team members.  
- 5: Enterprise licensed, integrated AI tools with support and enablement.

**Example survey prompts**

- Do you have access to AI tools that fit your daily content work?  
- Have you received training beyond basic vendor demos or intros?  
- Do you know how to use AI integrations in our CMS, DAM, or knowledge tools?

**Suggested KPIs**

- Percentage of content staff with access to approved AI tools  
- Percentage of staff who have completed AI training or enablement sessions  
- Number of live integrations between AI tools and core platforms (CMS, DAM, docs, wiki)  

---

## 3. Content quality and safeguards

Can you trust AI assisted content to meet quality, accuracy, and compliance standards?

**Scoring rubric**

- 1: No formal review or safeguard process for AI generated content.  
- 2: Review happens only when specific editors have time.  
- 3: Some repeatable checks, but not consistently followed.  
- 4: Documented human in the loop review process followed by most teams.  
- 5: Clear guardrails, review workflows, and documented standards across the organization.

**Example survey prompts**

- Do you trust AI outputs to be accurate without fact checking?  
- Are style and tone guides embedded in prompts, templates, or custom AI assistants?  
- Is there a defined process to catch biased, non inclusive, or non compliant language?

**Suggested KPIs**

- Percentage of AI assisted drafts that require major rewrites  
- Number of hallucination, accuracy, or compliance issues caught in review  
- Number of incidents where AI content required urgent rework post publication  

---

## 4. Productivity and efficiency

Is AI actually speeding things up without creating more churn?

**Scoring rubric**

- 1: No measurement of AI impact on productivity.  
- 2: Only anecdotal reports of time savings.  
- 3: Some tracked metrics, but no clear baseline.  
- 4: Before and after benchmarks exist for key content types.  
- 5: Regular reporting shows sustained efficiency improvements with AI.

**Example survey prompts**

- How much faster is drafting with AI compared to your previous workflow?  
- Are revisions more or less intensive when AI is involved?  
- Does AI reduce the time you spend staring at a blank page?

**Suggested KPIs**

- Average time to publish content assets before and after AI adoption  
- Draft to publish ratio for AI assisted versus non AI content  
- Average number of revision cycles per asset with and without AI  

---

## 5. Innovation and creativity

Is AI helping the team experiment, not just rewrite the same content faster?

**Scoring rubric**

- 1: AI only used to write first drafts or minor rewrites.  
- 2: Occasional use for brainstorming or outlines.  
- 3: AI used regularly for ideas, variations, and content experiments.  
- 4: AI enabling new formats, channels, or personalization that were not feasible before.  
- 5: AI powered experimentation is tracked, reviewed, and fed back into strategy.

**Example survey prompts**

- Have you used AI for brainstorming, concept generation, or headline variations?  
- Has AI enabled any content formats we could not support before?  
- Is there a process to record which AI driven experiments worked or failed?

**Suggested KPIs**

- Percentage of campaigns that used AI for ideation or concept development  
- Number of new content formats or pilots launched using AI  
- Engagement lift or conversion lift for AI personalized or AI assisted content  

---

## 6. Governance and compliance

Do you have clear rules and documentation for AI usage in content?

**Scoring rubric**

- 1: No AI policy for content or communication work.  
- 2: Policy exists in draft form or is not widely known.  
- 3: Policy is published, but enforcement is inconsistent.  
- 4: Policy is embedded into workflows and tools, and teams follow it.  
- 5: Governance is monitored, updated regularly, and aligned with legal and brand guidance.

**Example survey prompts**

- Do you know what our AI policy says about disclosure and attribution?  
- Are AI assisted pieces flagged or tagged differently in our approval workflows?  
- Are legal, risk, or compliance requirements explicitly considered before publishing AI assisted content?

**Suggested KPIs**

- Percentage of AI assisted content correctly flagged or tagged in CMS or DAM  
- Number of compliance or legal issues tied to AI generated or AI assisted drafts  
- Frequency of AI policy reviews and updates per year  

---

## 7. Adoption and culture

Is the team leaning into AI in a healthy way, or resisting it for fear or confusion?

**Scoring rubric**

- 1: Dominant sentiment is fear, skepticism, or resistance.  
- 2: Small group of early adopters, most others avoid AI.  
- 3: Split culture with a mix of enthusiasts and holdouts.  
- 4: Broad adoption with informal mentoring and knowledge sharing.  
- 5: Strong culture of responsible experimentation and peer support.

**Example survey prompts**

- Do you feel AI threatens your role, supports your work, or both?  
- Are there AI champions or power users who help others learn?  
- Do managers and leaders actively listen to feedback about AI workflows?

**Suggested KPIs**

- Percentage of staff who report using AI weekly in their content work  
- Percentage of staff trained by internal champions or peer sessions  
- Sentiment score from regular AI adoption surveys or pulse checks  

---

## 8. Metrics and reporting

Do you have a clear view of AI impact, beyond simple usage counts?

**Scoring rubric**

- 1: No AI specific reporting.  
- 2: Only basic usage data such as number of AI calls or logins.  
- 3: Some impact metrics, but not used consistently in decisions.  
- 4: Mix of leading and lagging indicators included in regular reports.  
- 5: AI metrics integrated into content and business reporting used for planning and prioritization.

**Example survey prompts**

- Do you see any reports that show AI contributions to our content work?  
- Are AI impact metrics used when making decisions about tools or processes?  
- Do our reports balance numbers with qualitative feedback from writers and editors?

**Suggested KPIs**

- Percentage of standard content or marketing reports that include AI contribution metrics  
- Number of decisions or initiatives explicitly informed by AI adoption data  
- Writer and editor satisfaction scores related to AI use in their workflow  

---

## Scoring model and heatmap

For each of the eight dimensions, assign a maturity score from 1 to 5:

- 1: Non existent or chaotic  
- 2: Emerging and mostly informal  
- 3: Defined but inconsistent  
- 4: Managed and measured  
- 5: Optimized and continuously improved  

Practical steps:

1. Run a workshop with key content stakeholders.  
2. For each dimension, discuss and agree on a score from 1 to 5.  
3. Capture notes that explain why you picked that score.  
4. Create a simple table or heatmap with dimensions as rows and scores as columns.  
5. Highlight any scores of 1 or 2 as immediate focus areas for remediation.  

Example table structure:

| Dimension                   | Score (1 to 5) | Notes and risks                          |
|----------------------------|----------------|-------------------------------------------|
| Strategic alignment        | 3              | Use cases defined but not mapped to KPIs |
| Capability readiness       | 4              | Good tools, training still uneven        |
| Content quality and safeguards | 2          | No formal review checklist yet           |

---

## Running assessments on a regular cadence

To keep this framework useful, treat it as a recurring check in rather than a one time exercise:

- Run a baseline assessment now and capture all scores and notes  
- Repeat the assessment every quarter or after major AI rollouts  
- Track which interventions changed scores over time  
- Use trends to inform training, governance, and tool investments  

Typical quarterly rhythm:

1. Re assess all eight dimensions with the same group of stakeholders.  
2. Compare scores with the previous quarter and flag changes.  
3. Update your AI policy, training plan, and content workflows based on gaps.  
4. Share improvements and remaining risks with leadership in a short summary.

---

## Customization

You can customize this framework for your organization by:

- Adding or removing survey prompts that match your culture  
- Swapping KPIs for metrics that align with your data stack  
- Extending the scoring rubric with more specific criteria per role or team  
- Linking each dimension to your internal policies, SLAs, and content standards  

If you keep the eight core dimensions and the 1 to 5 scoring model intact, you can still compare results across teams, regions, or business units over time.

